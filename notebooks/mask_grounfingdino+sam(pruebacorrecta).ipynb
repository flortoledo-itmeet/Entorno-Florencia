{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a246dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan librerías necesarias \n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict, Optional, Union, Tuple\n",
    "\n",
    "# Librerías para procesamiento de imágenes\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Modelos preentrenados para segmentación\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline, AutoModel\n",
    "from diffusers import StableDiffusionInpaintPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para representar un rectángulo delimitador en la imagen alrededor de un objeto\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox: \n",
    "    xmin: int \n",
    "    ymin: int\n",
    "    xmax: int\n",
    "    ymax: int\n",
    "    \n",
    "    # Devuelve una lista con los valores de las variables\n",
    "     \n",
    "    @property\n",
    "    def xyxy(self) -> List[float]:\n",
    "        return [self.xmin, self.ymin, self.xmax, self.ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase que representa un resultado de detección\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    score: float                        # Confianza del modelo en la detección del objeto\n",
    "    label: str                          # Etiqueta del objeto detectado\n",
    "    box: BoundingBox                    # Coordenadas del rectángulo\n",
    "    mask: Optional[np.array] = None     # Máscara de segmentación del objeto detectado\n",
    "\n",
    "    #  Método que toma un diccionario como argumento, que se espera que contenga la información de una detección\n",
    "    #  Devuelve una instancia de la clase 'DetectionResult'\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n",
    "        return cls(score=detection_dict['score'],\n",
    "                   label=detection_dict['label'],\n",
    "                   box=BoundingBox(xmin=detection_dict['box']['xmin'],\n",
    "                                   ymin=detection_dict['box']['ymin'],\n",
    "                                   xmax=detection_dict['box']['xmax'],\n",
    "                                   ymax=detection_dict['box']['ymax']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga una imagen desde una URL o desde el disco\n",
    "\n",
    "def load_image(image_str: str) -> Image.Image:\n",
    "    if image_str.startswith(\"http\"):\n",
    "        image = Image.open(requests.get(image_str, stream=True).raw).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_str).convert(\"RGB\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae las bounding boxes de las detecciones\n",
    "\n",
    "def get_boxes(results: List[DetectionResult]) -> List[List[List[float]]]:\n",
    "    return [[result.box.xyxy for result in results]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte las máscaras en listas de arrays numpy\n",
    "\n",
    "def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:\n",
    "    masks = masks.cpu().float()\n",
    "    masks = masks.permute(0, 2, 3, 1).mean(axis=-1)\n",
    "    masks = (masks > 0).int().cpu().numpy().astype(np.uint8)\n",
    "    masks = list(masks)  # Lista de máscaras individuales\n",
    "\n",
    "    if polygon_refinement:\n",
    "        for idx, mask in enumerate(masks):\n",
    "            polygon = mask_to_polygon(mask)\n",
    "            masks[idx] = polygon_to_mask(polygon, mask.shape)\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte una máscara binaria en un polígono (contorno más grande)\n",
    "\n",
    "def mask_to_polygon(mask: np.ndarray) -> List[List[int]]:\n",
    "    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    return largest_contour.reshape(-1, 2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte un polígono en una máscara binaria\n",
    "\n",
    "def polygon_to_mask(polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]) -> np.ndarray:\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "    pts = np.array(polygon, dtype=np.int32)\n",
    "    cv2.fillPoly(mask, [pts], color=(255,))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devuelve una imagen donde la región de la máscara está en negro y el resto en blanco \n",
    "\n",
    "def isolate_mask_region(image: np.ndarray, mask: np.ndarray, invert: bool = False) -> np.ndarray:\n",
    "    mask = mask.astype(np.uint8)\n",
    "    if invert:\n",
    "        mask = cv2.bitwise_not(mask)\n",
    "\n",
    "    white_background = np.ones_like(image) * 255\n",
    "    if len(mask.shape) == 2:\n",
    "        mask_3ch = np.stack([mask]*3, axis=-1)\n",
    "    else:\n",
    "        mask_3ch = mask\n",
    "\n",
    "    result = cv2.bitwise_and(white_background, 255 - mask_3ch)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecta objetos en una imagen usando Grounding DINO con etiquetas personalizadas\n",
    "\n",
    "def detect(image: Image.Image, labels: List[str], threshold: float = 0.3, detector_id: Optional[str] = None) -> List[DetectionResult]:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    detector_id = detector_id or \"IDEA-Research/grounding-dino-tiny\"\n",
    "    object_detector = pipeline(model=detector_id, task=\"zero-shot-object-detection\", device=device)\n",
    "    labels = [label if label.endswith(\".\") else label + \".\" for label in labels]\n",
    "    results = object_detector(image, candidate_labels=labels, threshold=threshold)\n",
    "    return [DetectionResult.from_dict(result) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica SAM (Segment Anything Model) para generar máscaras de los objetos detectados\n",
    "\n",
    "def segment(image: Image.Image, detection_results: List[DetectionResult], polygon_refinement: bool = False, segmenter_id: Optional[str] = None) -> List[DetectionResult]:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    segmenter_id = segmenter_id or \"facebook/sam-vit-base\"\n",
    "    segmentator = AutoModel.from_pretrained(segmenter_id).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(segmenter_id)\n",
    "\n",
    "    boxes = get_boxes(detection_results)\n",
    "    inputs = processor(images=image, input_boxes=boxes, return_tensors=\"pt\").to(device)\n",
    "    outputs = segmentator(**inputs)\n",
    "    masks = processor.post_process_masks(\n",
    "        masks=outputs.pred_masks,\n",
    "        original_sizes=inputs.original_sizes,\n",
    "        reshaped_input_sizes=inputs.reshaped_input_sizes\n",
    "    )[0]\n",
    "\n",
    "    masks = refine_masks(masks, polygon_refinement)\n",
    "    for detection_result, mask in zip(detection_results, masks):\n",
    "        detection_result.mask = mask\n",
    "\n",
    "    return detection_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d340b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "    Pipeline completo:\n",
    "    - Carga la imagen\n",
    "    - Detecta objetos\n",
    "    - Segmenta objetos detectados\n",
    "    - Devuelve la imagen y las detecciones con sus máscaras\n",
    "\"\"\"\n",
    "def grounded_segmentation(\n",
    "    image: Union[Image.Image, str],\n",
    "    labels: List[str],\n",
    "    threshold: float = 0.3,\n",
    "    polygon_refinement: bool = False,\n",
    "    detector_id: Optional[str] = None,\n",
    "    segmenter_id: Optional[str] = None\n",
    ") -> Tuple[np.ndarray, List[DetectionResult]]:\n",
    "    if isinstance(image, str):\n",
    "        image = load_image(image)\n",
    "\n",
    "    detections = detect(image, labels, threshold, detector_id)\n",
    "    detections = segment(image, detections, polygon_refinement, segmenter_id)\n",
    "\n",
    "    return np.array(image), detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c33f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../data/istockphoto-1200677760-612x612.jpg\"\n",
    "labels = [\"a hair\"]\n",
    "threshold = 0.3\n",
    "detector_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "segmenter_id = \"facebook/sam-vit-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar y segmentar\n",
    "\n",
    "image_array, detections = grounded_segmentation(\n",
    "    image=image_path,\n",
    "    labels=labels,\n",
    "    threshold=threshold,\n",
    "    polygon_refinement=True,\n",
    "    detector_id=detector_id,\n",
    "    segmenter_id=segmenter_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d81af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la máscara del cabello (primera detección)\n",
    "hair_mask = detections[0].mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4740a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar imagen blanco + cabello negro\n",
    "output_image = isolate_mask_region(image_array, hair_mask, invert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d3e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mostrar el resultado\n",
    "plt.imshow(output_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c424fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.fromarray(output_image)\n",
    "image.save(\"../data/mask.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
